# -*- coding: utf-8 -*-
"""FraudDetectionPySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QlwY46RFUkSdtQD11y-OiK7F8sTtwzXe
"""




# importing the required pyspark library
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import col, sum, when, lag, abs, sqrt, pow, floor, current_date, datediff, to_timestamp, hour
from pyspark.sql.window import Window





def age_segment(df):
  # Calculate age based on current date and DOB
  df = df.withColumn("age", floor(datediff(current_date(), col("dob")) / 365))

  # Create the "age_group" column based on the given logic
  df = df.withColumn(
      "age_group",
      when(col("age") < 18, "under 18")
      .when((col("age") >= 18) & (col("age") <= 30), "18-30")
      .when((col("age") >= 31) & (col("age") <= 50), "31-50")
      .when(col("age") >= 50, "over 50")
      .otherwise("Unknown")
  )
  return df

def pop_segment(df):
  # Creating the "city_pop_segment" column based on the given logic
  df = df.withColumn(
      "city_pop_segment",
      when(col("city_pop") < 10000, "few density")
      .when((col("city_pop") > 10000) & (col("city_pop") < 50000), "normal density")
      .when(col("city_pop") > 50000, "high density")
      .otherwise("Unknown")
  )

  return df

def unix_segment(df):
  # Converting "most_recent" time from sec to mins.
  df = df.withColumn("most_recent", (col("most_recent") / 3600.0))

  # Creating the "recent_segment" by dividing the day into 4 section and default values to most_recent_transaction for < 1
  df = df.withColumn(
      "recenct_segment",
      when(col("most_recent") < 1, "most_recent_transaction")
      .when((col("most_recent") > 1) & (col("most_recent") < 6), "within 6 hours")
      .when((col("most_recent") > 6) & (col("most_recent") < 12), "after 6 hours")
      .when((col("most_recent") > 12) & (col("most_recent") < 24), "after half-Day")
      .when(col("most_recent") > 24, "after 24 hours")
      .otherwise("first transaction")
  )
  return df

def feature_engineering(df):
  # Defining a window specification partitioned by 'cc_num' and ordered by 'unix_time'
  window_spec = Window.partitionBy("cc_num").orderBy("unix_time")

  # Compute the difference between consecutive 'unix_time' values within each group
  df = df.withColumn("most_recent", col("unix_time") - lag("unix_time", 1).over(window_spec))
  df = df.withColumn("most_recent", when(col("most_recent").isNull(), -1).otherwise(col("most_recent")))

  # Calculatig absolute differences between 'lat' and 'merch_lat'
  df = df.withColumn("diff_lat", abs(col("lat") - col("merch_lat")))
  # Calculating absolute differences between 'long' and 'merch_long'
  df = df.withColumn("diff_long", abs(col("long") - col("merch_long")))

  # since the lat and long difference for each degree is equal to 110 kilometers(approximately) and using pythogorous formula
  # Calculating the "displacement" column using the formula
  df = df.withColumn("displacement",sqrt(pow(col("diff_lat") * 110, 2) + pow(col("diff_long") * 110, 2)))

  # Converting the timestamp column to a time column
  df = df.withColumn("hour", hour(to_timestamp(col("trans_date_trans_time"), "yyyy-MM-dd HH:mm:ss")))

  #segmented values
  df = pop_segment(df)
  df = age_segment(df)
  df = unix_segment(df)




  #dropping columns that are not useful for the model with no unique patterns
  #
  columns_ = ("_c0","first","last","dob","unix_time","city","zip","street", "lat", "long", "merch_lat","merch_long","trans_date_trans_time","cc_num","merchant","job","state")

  df = df.drop(*columns_)

  return df


if __name__ == "__main__":
    # create spark session
    spark_session = SparkSession.builder.appName('_FraudDetection').getOrCreate()





    # from gcp bucket
    input_path_train = 'gs://dataproc-staging-us-central1-441320292389-riaohl70/fraud_data/fraudTrain.csv'
    input_path_test = 'gs://dataproc-staging-us-central1-441320292389-riaohl70/fraud_data/fraudTest.csv'

    



    #CSV file can be downloaded from the link mentioned above.
    data_train = spark_session.read.csv(input_path_train,
                        inferSchema=True,header=True) # , schema=schema #if schema needed

    data_test = spark_session.read.csv(input_path_train,
                        inferSchema=True,header=True)



    
    # performing our pre feature selection and engineering
    ready_train = feature_engineering(data_train)
    ready_test = feature_engineering(data_test)
    
    
    _ = ready_train.select("age_group","most_recent","city_pop_segment","displacement","hour")
    
    
    print("the segmentations from the existing features are :\n")
    _.show()

    